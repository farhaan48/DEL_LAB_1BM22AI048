{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqQyDBvI91NL",
        "outputId": "78e87803-fb7b-4cef-9016-6830dd356292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final predictions:\n",
            "Input: [0 0], Prediction: 0.0\n",
            "Input: [0 1], Prediction: 1.0\n",
            "Input: [1 0], Prediction: 1.0\n",
            "Input: [1 1], Prediction: 0.0\n"
          ]
        }
      ],
      "source": [
        "#How does a feed forward neural network with multi layer perceptron architecture solve the XOR problem.\n",
        "\n",
        "import numpy as np\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "# XOR input and output\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "# Initialize weights\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 2\n",
        "output_layer_size = 1\n",
        "# Random weights initialization\n",
        "weights_input_hidden = np.random.uniform(-1, 1, (input_layer_size, hidden_layer_size))\n",
        "weights_hidden_output = np.random.uniform(-1, 1, (hidden_layer_size, output_layer_size))\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "# Training the MLP\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_input = np.dot(X, weights_input_hidden)\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "    # Backpropagation\n",
        "    error = y - predicted_output\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "    # Update weights\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
        "# Predictions after training\n",
        "print(\"Final predictions:\")\n",
        "for i in range(len(X)):\n",
        "    hidden_layer_output = sigmoid(np.dot(X[i], weights_input_hidden))\n",
        "    predicted_output = sigmoid(np.dot(hidden_layer_output, weights_hidden_output))\n",
        "    print(f\"Input: {X[i]}, Prediction: {np.round(predicted_output[0])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#how does a feed forward neural network handle multi class multification tasks and what are the key steps involved in propogating input network to the network to produce classs probabilities using activation function like softmax\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder()\n",
        "y_onehot = encoder.fit_transform(y).toarray()  # Convert to array\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "# Initialize parameters\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 5\n",
        "output_size = y_onehot.shape[1]\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "bias_hidden = np.random.randn(hidden_size)\n",
        "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "bias_output = np.random.randn(output_size)\n",
        "# Activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
        "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_input = np.dot(X_train, weights_input_hidden) + bias_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "    predicted_output = softmax(output_layer_input)\n",
        "    # Compute loss (cross-entropy)\n",
        "    loss = -np.mean(np.sum(y_train * np.log(predicted_output + 1e-9), axis=1))  # Small constant for stability\n",
        "    # Backpropagation\n",
        "    output_error = predicted_output - y_train\n",
        "    hidden_error = np.dot(output_error, weights_hidden_output.T) * hidden_layer_output * (1 - hidden_layer_output)\n",
        "    # Update weights and biases\n",
        "    weights_hidden_output -= learning_rate * np.dot(hidden_layer_output.T, output_error) / X_train.shape[0]\n",
        "    bias_output -= learning_rate * np.mean(output_error, axis=0)\n",
        "    weights_input_hidden -= learning_rate * np.dot(X_train.T, hidden_error) / X_train.shape[0]\n",
        "    bias_hidden -= learning_rate * np.mean(hidden_error, axis=0)\n",
        "    # Print loss every 1000 epochs\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
        "# Predictions on test set\n",
        "hidden_layer_input = np.dot(X_test, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "predicted_output = softmax(output_layer_input)\n",
        "# Get predicted class labels\n",
        "predicted_classes = np.argmax(predicted_output, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "# Accuracy\n",
        "accuracy = np.mean(predicted_classes == true_classes)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEOW5HBE-sKW",
        "outputId": "0214bed3-e980-47b5-ef14-d326f9dcc39d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.3636\n",
            "Epoch 1000, Loss: 0.7236\n",
            "Epoch 2000, Loss: 0.6074\n",
            "Epoch 3000, Loss: 0.5534\n",
            "Epoch 4000, Loss: 0.5252\n",
            "Epoch 5000, Loss: 0.5087\n",
            "Epoch 6000, Loss: 0.4981\n",
            "Epoch 7000, Loss: 0.4904\n",
            "Epoch 8000, Loss: 0.4839\n",
            "Epoch 9000, Loss: 0.4759\n",
            "Accuracy: 96.67%\n"
          ]
        }
      ]
    }
  ]
}