{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Step 1: Load and preprocess the dataset\n",
        "# Load the IMDB dataset with top 10,000 most common words\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
        "# Pad sequences to ensure uniform input size\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_length)\n",
        "# Step 2: Build the RNN model\n",
        "model = Sequential([\n",
        "Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
        "SimpleRNN(128, activation='tanh', return_sequences=False),\n",
        "Dropout(0.5),\n",
        "Dense(64, activation='relu'),\n",
        "Dropout(0.5),\n",
        "Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Step 3: Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy'])\n",
        "# Step 4: Train the model\n",
        "history = model.fit(x_train, y_train,\n",
        "epochs=5,\n",
        "batch_size=64,\n",
        "validation_data=(x_test, y_test))\n",
        "# Step 5: Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "# Step 6: Predict sentiment for a sample review\n",
        "sample_review = \"This movie was fantastic! The characters were well-developed and the plot was thrilling.\"\n",
        "# Tokenize and pad the sample review\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = imdb.get_word_index()\n",
        "reverse_word_index = {value: key for (key, value) in tokenizer.items()}\n",
        "encoded_review = [tokenizer.get(word, 2) for word in sample_review.lower().split()]\n",
        "padded_review = pad_sequences([encoded_review], maxlen=max_length)\n",
        "# Predict sentiment\n",
        "prediction = model.predict(padded_review)\n",
        "sentiment = \"Positive\" if prediction[0] > 0.5 else \"Negative\"\n",
        "print(f\"Predicted Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdoI7QmHBvJQ",
        "outputId": "c1dbd5e8-7235-4447-a29f-bf45b50ddf83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 167ms/step - accuracy: 0.5114 - loss: 0.7310 - val_accuracy: 0.4992 - val_loss: 0.6947\n",
            "Epoch 2/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 186ms/step - accuracy: 0.5062 - loss: 0.7067 - val_accuracy: 0.5086 - val_loss: 0.6917\n",
            "Epoch 3/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 167ms/step - accuracy: 0.5239 - loss: 0.6917 - val_accuracy: 0.6148 - val_loss: 0.6592\n",
            "Epoch 4/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 166ms/step - accuracy: 0.6527 - loss: 0.6191 - val_accuracy: 0.7776 - val_loss: 0.4918\n",
            "Epoch 5/5\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 186ms/step - accuracy: 0.7953 - loss: 0.4655 - val_accuracy: 0.7875 - val_loss: 0.4530\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7824 - loss: 0.4567\n",
            "Test Accuracy: 0.79\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    }
  ]
}